# Differential_Privacy_in_Fraud_Transaction_Detection

This project tackles the delicate balance between data privacy and fraud detection by focusing on safeguarding individual privacy within aggregate statistics, a crucial issue in the data privacy landscape. The challenge is to prevent the accidental disclosure of sensitive personal information from aggregated data releases, which necessitates the adoption of differential privacy methods. The project's approach involves integrating differential privacy techniques into the process of detecting fraudulent transactions. By introducing controlled noise into transaction data, these methods protect the details of individual transactions while still allowing for effective fraud analysis. The project leverages specialized machine learning models, trained on both anonymized (privatized) and original (non-privatized) data, to detect fraud. The effectiveness of these models is assessed using metrics such as precision, recall, and F1-score, comparing their performance on privatized versus non-privatized datasets. This research highlights the effectiveness of differential privacy techniques in enhancing privacy without undermining fraud detection efficiency, showcasing the critical interplay between maintaining privacy and implementing strong security measures in fraud prevention strategies.


The data source is a synthetic dataset of credit card transactions generated using the ”Faker” Python library.
